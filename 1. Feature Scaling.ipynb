{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d38fb3c",
   "metadata": {},
   "source": [
    "# <span style = \"color:coral\">Feature scaling </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2ea1f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0900f3d7",
   "metadata": {},
   "source": [
    "Feature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9368f",
   "metadata": {},
   "source": [
    "I’m sure most of you must have faced this issue in your projects or your learning journey. For example, one feature is entirely in kilograms while the other is in grams, another one is liters, and so on. How can we use these features when they vary so vastly in terms of what they’re presenting?\n",
    "\n",
    "This is where I turned to the concept of feature scaling. It’s a crucial part of the data preprocessing stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a92af",
   "metadata": {},
   "source": [
    "### Why Should we Use Feature Scaling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bf9255",
   "metadata": {},
   "source": [
    "The first question we need to address – why do we need to scale the variables in our dataset? Some machine learning algorithms are sensitive to feature scaling while others are virtually invariant to it. Let me explain that in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f1551f",
   "metadata": {},
   "source": [
    "#### 1. Gradient Descent Based Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c81916",
   "metadata": {},
   "source": [
    "    Machine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0805540",
   "metadata": {},
   "source": [
    "#### 2. Distance-Based Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d413be76",
   "metadata": {},
   "source": [
    "    Distance algorithms like KNN, K-means, and SVM are most affected by the range of features. This is because behind the scenes they are using distances between data points to determine their similarity.Since both the features have different scales, there is a chance that higher weightage is given to features with higher magnitude. This will impact the performance of the machine learning algorithm and obviously, we do not want our algorithm to be biassed towards one feature.\n",
    "    \n",
    "    Therefore, we scale our data before employing a distance based algorithm so that all the features contribute equally to the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57893565",
   "metadata": {},
   "source": [
    "#### 3. Tree-Based Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f666d",
   "metadata": {},
   "source": [
    "    Tree-based algorithms, on the other hand, are fairly insensitive to the scale of the features. Think about it, a decision tree is only splitting a node based on a single feature. The decision tree splits a node on a feature that increases the homogeneity of the node. This split on a feature is not influenced by other features.\n",
    "    \n",
    "    So, there is virtually no effect of the remaining features on the split. This is what makes them invariant to the scale of the features!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5fff29",
   "metadata": {},
   "source": [
    "### Normalization vs Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d99c7e",
   "metadata": {},
   "source": [
    "#### What is Normalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a2397",
   "metadata": {},
   "source": [
    "Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334b620",
   "metadata": {},
   "source": [
    "#### What is Standardization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b77b3",
   "metadata": {},
   "source": [
    "Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68234948",
   "metadata": {},
   "source": [
    "### The Big Question – Normalize or Standardize?\n",
    "Normalization vs. standardization is an eternal question among machine learning newcomers. Let me elaborate on the answer in this section.\n",
    "\n",
    "* Normalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks.\n",
    "* Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Also, unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e575ba",
   "metadata": {},
   "source": [
    "## Normalization using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab404f",
   "metadata": {},
   "source": [
    "#### <span style = \"color:red\">Before proceeding with Feature scaling, Split the dataset into X and y. X being the input variables and y the target</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f5189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data normalization with sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create normalization model\n",
    "norm = MinMaxScaler()\n",
    "\n",
    "# Store column names of X in a variable\n",
    "xcolumns = X.columns\n",
    "\n",
    "# fit_transform data into a new variable\n",
    "X_scaled = norm.fit_transform(X)\n",
    "\n",
    "# Create a new DataFrame with X and assign the column names we fetched earlier\n",
    "X_scaled = pd.DataFrame(X_scaled, columns = xcolumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bfa876",
   "metadata": {},
   "source": [
    "## Standardization using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe92ff5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Store column names of X in a variable\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m xcolumns \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# fit_transform data into a new variable\u001b[39;00m\n\u001b[0;32m     11\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "     # data standardization with  sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create scaler model\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Store column names of X in a variable\n",
    "xcolumns = X.columns\n",
    "\n",
    "# fit_transform data into a new variable\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create a new DataFrame with X and assign the column names we fetched earlier\n",
    "X_scaled = pd.DataFrame(X_scaled, columns = xcolumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2d1668",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
